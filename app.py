# -*- coding: utf-8 -*-
"""APP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fw1DuR8GjrFMJBk0f5iF2aI_uQNruPoQ
"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Import Required Libraries
import pandas as pd       # For data handling
import numpy as np        # For numerical operations
import matplotlib.pyplot as plt  # For plotting
import seaborn as sns     # For advanced plotting

# Enable plots inside the notebook
# %matplotlib inline

# Step 2: Load the dataset
df = pd.read_csv('/content/creditcard.csv')  # Path will work after upload

# Step 3: Show first 5 rows of data
df.head()

# Check basic structure
df.info()

# Shape of the dataset
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

# Summary of numeric features
df.describe()

# Check how many fraud and non-fraud transactions
print("Class distribution:")
print(df['Class'].value_counts())

# Plot the class distribution
sns.countplot(x='Class', data=df)
plt.title("Class Distribution\n(0: Legit | 1: Fraud)")
plt.show()

"""# Data Preprocessing"""

from sklearn.preprocessing import StandardScaler

# Create a copy of the data
data = df.copy()

# Standardize 'Amount' and 'Time' features
scaler = StandardScaler()
data['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data['Time'] = scaler.fit_transform(data['Time'].values.reshape(-1, 1))

# Check the scaled values
data[['Time', 'Amount']].describe()

# Separate features and target
X = data.drop('Class', axis=1)
y = data['Class']

from sklearn.model_selection import train_test_split

# Split the data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Confirm shapes
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# Check for missing values
print("Missing values in dataset:")
print(data.isnull().sum())

# Drop rows with NaN values (if any)
data = data.dropna()

# Separate features and target again
X = data.drop('Class', axis=1)
y = data['Class']

# Now split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""# Handle Class Imbalance using SMOTE"""

# Install imbalanced-learn library (only once)
!pip install -U imbalanced-learn

# Import SMOTE
from imblearn.over_sampling import SMOTE

# Apply SMOTE only on training data
sm = SMOTE(random_state=42)
X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)

# Print new class distribution
print("After SMOTE:")
print(y_train_sm.value_counts())

"""# Train Your First Model (Logistic Regression)"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train_sm, y_train_sm)

# Make predictions on test data
y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

"""# Try with Random Forest (A More Powerful Model)"""

from sklearn.ensemble import RandomForestClassifier

# Train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_sm, y_train_sm)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the Random Forest model
print("Confusion Matrix (Random Forest):")
print(confusion_matrix(y_test, y_pred_rf))

print("\nClassification Report (Random Forest):")
print(classification_report(y_test, y_pred_rf))

print("\nAccuracy Score (Random Forest):", accuracy_score(y_test, y_pred_rf))

# Install Streamlit
!pip install streamlit

import joblib

# Save the model
joblib.dump(rf_model, 'credit_card_fraud_model.pkl')

# Create a simple Streamlit app for fraud detection

import streamlit as st
import numpy as np
import pandas as pd
import joblib

# Load the pre-trained model
model = joblib.load('credit_card_fraud_model.pkl')

# App title
st.title("Credit Card Fraud Detection")

# User input for prediction (example)
st.write("Enter Transaction Data:")

# Create input fields for the user
time = st.number_input('Time', min_value=-1000, max_value=10000, value=0)
amount = st.number_input('Amount', min_value=0.0, max_value=100000.0, value=0.0)

# Button to make prediction
if st.button('Predict'):
    # Prepare the input data
    input_data = np.array([[time, amount]])
    prediction = model.predict(input_data)

    # Display the result
    if prediction[0] == 1:
        st.write("Prediction: Fraud")
    else:
        st.write("Prediction: Legit")